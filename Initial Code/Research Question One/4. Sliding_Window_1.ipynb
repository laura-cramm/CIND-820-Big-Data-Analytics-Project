{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHhQO9W4nHx2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import statistics\n",
        "import random\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q1features_train_1 = pd.read_pickle(\"q1features_train_1.pickle\")\n",
        "q1features_valid_1 = pd.read_pickle(\"q1features_valid_1.pickle\")\n",
        "q1target_train_1 = pd.read_pickle(\"q1target_train_1.pickle\")\n",
        "q1target_valid_1 = pd.read_pickle(\"q1target_valid_1.pickle\")"
      ],
      "metadata": {
        "id": "jSENDBw7nSen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputation of missing data was performed. The alternative was to use complete case analysis, which involves removing from the dataset any rows that were missing any attributes. Complete case analysis leads to a loss of information and can lead to bias if the missing data is not missing completely at random.\n",
        "\n",
        "Single-imputation methods were used; missing categorical values were imputed using the mode of that attribute and missing numeric values were imputed using the median. The validation data set was imputed using the mode and median values of the training data set. To avoid data leakage, the training data set was not imputed using validation set data. Data leakage can lead to inaccurately high model performance (but comparatively poor performance on the test data)."
      ],
      "metadata": {
        "id": "33P24Vqb53hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imputing validation set\n",
        "for col in q1features_valid_1.columns.tolist():\n",
        "  q1features_valid_1[col] = q1features_valid_1[col].replace([None], np.nan)\n",
        "  if (q1features_valid_1[col].dtype == 'category' or q1features_valid_1[col].dtype =='datetime64[ns]'): #\n",
        "    q1features_valid_1[col]= q1features_valid_1[col].fillna(random.choice(statistics.multimode(q1features_train_1[col])))\n",
        "  if (q1features_valid_1[col].dtype == 'Int64' or q1features_valid_1[col].dtype == 'int64'):\n",
        "    q1features_valid_1[col] = q1features_valid_1[col].astype('float64')\n",
        "  if (q1features_valid_1[col].dtype == 'float64'):\n",
        "    q1features_valid_1[col] = q1features_valid_1[col].fillna(q1features_train_1[col].median())"
      ],
      "metadata": {
        "id": "FaTrQRtNfwAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Imputing training set\n",
        "for col in q1features_train_1.columns.tolist():\n",
        "  q1features_train_1[col] = q1features_train_1[col].replace([None], np.nan)\n",
        "  if (q1features_train_1[col].dtype == 'category' or q1features_train_1[col].dtype =='datetime64[ns]'): #\n",
        "    q1features_train_1[col] = q1features_train_1[col].fillna(random.choice(statistics.multimode(q1features_train_1[col])))\n",
        "  if (q1features_train_1[col].dtype == 'Int64' or q1features_train_1[col].dtype == 'int64'):\n",
        "    q1features_train_1[col] = q1features_train_1[col].astype('float64')\n",
        "  if (q1features_train_1[col].dtype == 'float64'):\n",
        "    q1features_train_1[col] = q1features_train_1[col].fillna(q1features_train_1[col].median())"
      ],
      "metadata": {
        "id": "JZczKowifxdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numeric attributes were standardized; variable importance will be assessed using regression models, and standardization allows for easier interpretation of the model coefficients. Standardization was performed instead of normalization, as standardization is more resistant to outliers."
      ],
      "metadata": {
        "id": "R8vHnx4-5-_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardization of validation set\n",
        "for col in q1features_valid_1.columns.tolist():\n",
        "  if (q1features_valid_1[col].dtype == 'Int64' or q1features_valid_1[col].dtype == 'int64' or q1features_valid_1[col].dtype == 'float64'):\n",
        "    for i in range(len(q1features_valid_1[col])):\n",
        "      q1features_valid_1.loc[q1features_valid_1.index[i], col] = (q1features_valid_1.loc[q1features_valid_1.index[i], col] - q1features_train_1[col].mean())/np.std(q1features_train_1[col])"
      ],
      "metadata": {
        "id": "WsEZmFgJf0IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardization  of training set\n",
        "for col in q1features_train_1.columns.tolist():\n",
        "  if (q1features_train_1[col].dtype == 'Int64' or q1features_train_1[col].dtype == 'int64' or q1features_train_1[col].dtype == 'float64'):\n",
        "    for i in range(len(q1features_train_1[col])):\n",
        "      q1features_train_1.loc[q1features_train_1.index[i], col] = (q1features_train_1.loc[q1features_train_1.index[i], col] - q1features_train_1[col].mean())/np.std(q1features_train_1[col])"
      ],
      "metadata": {
        "id": "ZbnChiwCf2XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The SMOTE algorithm cannot handle datetime, so converting to number of\n",
        "#days since January 1st, year one\n",
        "for i in range(len(q1features_train_1['INTERVIEWDATE'])):\n",
        "  q1features_train_1.loc[q1features_train_1.index[i], 'INTERVIEWDATE'] =  q1features_train_1.loc[q1features_train_1.index[i], 'INTERVIEWDATE'].toordinal()\n",
        "\n",
        "for i in range(len(q1features_valid_1['INTERVIEWDATE'])):\n",
        "  q1features_valid_1.loc[q1features_valid_1.index[i], 'INTERVIEWDATE'] =  q1features_valid_1.loc[q1features_valid_1.index[i], 'INTERVIEWDATE'].toordinal()\n",
        "\n",
        "q1features_train_1['INTERVIEWDATE'] = q1features_train_1['INTERVIEWDATE'].astype('float64')\n",
        "q1features_valid_1['INTERVIEWDATE'] = q1features_valid_1['INTERVIEWDATE'].astype('float64')"
      ],
      "metadata": {
        "id": "xWhG4TBxf3kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting categorical variables to dummy variables\n",
        "for col in q1features_train_1.columns.tolist():\n",
        "  if (q1features_train_1[col].dtype == 'category'):\n",
        "    q1features_train_1 = pd.concat([q1features_train_1, pd.get_dummies(q1features_train_1[col], prefix=col, drop_first=True, dtype='float')], axis=1)\n",
        "    q1features_train_1 = q1features_train_1.drop([col], axis=1)"
      ],
      "metadata": {
        "id": "CBuRUrnEf5IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in q1features_valid_1.columns.tolist():\n",
        "  if (q1features_valid_1[col].dtype == 'category'):\n",
        "    q1features_valid_1 = pd.concat([q1features_valid_1, pd.get_dummies(q1features_valid_1[col], prefix=col, drop_first=True, dtype='float')], axis=1)\n",
        "    q1features_valid_1 = q1features_valid_1.drop([col], axis=1)"
      ],
      "metadata": {
        "id": "b3Y6cRiSf7C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1target_train_1 = q1target_train_1.astype('bool')\n",
        "q1target_train_1 = q1target_train_1.astype('float')"
      ],
      "metadata": {
        "id": "wdTGLzR7f8-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1target_valid_1 = q1target_valid_1.astype('bool')\n",
        "q1target_valid_1 = q1target_valid_1.astype('float')"
      ],
      "metadata": {
        "id": "l4IYSQfgf-aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multicollinearity can lead to model overfitting and can artificially hide the importance of explanatory variables. Multicollinearity (and collinearity) can be assessed using variance inflation factor values. A VIF value of over 10 indicates serious multicollinearity (Vittinghoff, E., Shiboski, S., Glidden, D., & McCulloch, C. (2004). Regression Methods in Biostatistics: Linear, Logistic, Survival and Repeated Measures Models. New York:Springer. https://doi.org/10.1007/b138825). Independent variables were iteratively removed from the model starting with the variable with the highest VIF value until all VIF values were below an accceptable threshold."
      ],
      "metadata": {
        "id": "Mfsj92bY6Ixm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing highly correlated variables\n",
        "vifcalcs=q1features_train_1"
      ],
      "metadata": {
        "id": "7e78aaMAgEeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = vifcalcs.columns\n",
        "\n",
        "# calculating VIF for each feature\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(vifcalcs.values, i)\n",
        "                          for i in range(len(vifcalcs.columns))]"
      ],
      "metadata": {
        "id": "ZNsLM0MTgF5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (len(vif_data['VIF'])):\n",
        "  if (vif_data.loc[vif_data.index[i], 'VIF'])> 10:\n",
        "    print(vif_data.loc[vif_data.index[i], 'feature'],vif_data.loc[vif_data.index[i], 'VIF'])"
      ],
      "metadata": {
        "id": "UuV8FFVegG-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unwanted = vifcalcs.columns[vifcalcs.columns.str.startswith('PRIMINSR')]\n",
        "#vifcalcs.drop(unwanted, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "VbJkSD-agK79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unwanted = q1features_train_1.columns[q1features_train_1.columns.str.startswith('IDAY')]\n",
        "q1features_train_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_train_1.columns[q1features_train_1.columns.str.startswith('QSTVER')]\n",
        "q1features_train_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_train_1.columns[q1features_train_1.columns.str.startswith('STATE')]\n",
        "q1features_train_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_train_1.columns[q1features_train_1.columns.str.startswith('_AGE65YR')]\n",
        "q1features_train_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_train_1.columns[q1features_train_1.columns.str.startswith('IMONTH')]\n",
        "q1features_train_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_train_1.columns[q1features_train_1.columns.str.startswith('INTERVIEWDATE')]\n",
        "q1features_train_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_train_1.columns[q1features_train_1.columns.str.startswith('_AGE_G')]\n",
        "q1features_train_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_train_1.columns[q1features_train_1.columns.str.startswith('_AGE80')]\n",
        "q1features_train_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_train_1.columns[q1features_train_1.columns.str.startswith('HTM4')]\n",
        "q1features_train_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_train_1.columns[q1features_train_1.columns.str.startswith('WTKG3')]\n",
        "q1features_train_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_train_1.columns[q1features_train_1.columns.str.startswith('USENOW3')]\n",
        "q1features_train_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_train_1.columns[q1features_train_1.columns.str.startswith('EDUCA')]\n",
        "q1features_train_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_train_1.columns[q1features_train_1.columns.str.startswith('SLEPTIM1')]\n",
        "q1features_train_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_train_1.columns[q1features_train_1.columns.str.startswith('PRIMINSR')]\n",
        "q1features_train_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_train_1.columns[q1features_train_1.columns.str.startswith('FMONTH')]\n",
        "q1features_train_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_train_1.columns[q1features_train_1.columns.str.startswith('SMOKE100')]\n",
        "q1features_train_1.drop(unwanted, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "YofYE6E_gLfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unwanted = q1features_valid_1.columns[q1features_valid_1.columns.str.startswith('IDAY')]\n",
        "q1features_valid_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_valid_1.columns[q1features_valid_1.columns.str.startswith('QSTVER')]\n",
        "q1features_valid_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_valid_1.columns[q1features_valid_1.columns.str.startswith('STATE')]\n",
        "q1features_valid_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_valid_1.columns[q1features_valid_1.columns.str.startswith('_AGE65YR')]\n",
        "q1features_valid_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_valid_1.columns[q1features_valid_1.columns.str.startswith('IMONTH')]\n",
        "q1features_valid_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_valid_1.columns[q1features_valid_1.columns.str.startswith('INTERVIEWDATE')]\n",
        "q1features_valid_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_valid_1.columns[q1features_valid_1.columns.str.startswith('_AGE_G')]\n",
        "q1features_valid_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_valid_1.columns[q1features_valid_1.columns.str.startswith('_AGE80')]\n",
        "q1features_valid_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_valid_1.columns[q1features_valid_1.columns.str.startswith('HTM4')]\n",
        "q1features_valid_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_valid_1.columns[q1features_valid_1.columns.str.startswith('WTKG3')]\n",
        "q1features_valid_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_valid_1.columns[q1features_valid_1.columns.str.startswith('USENOW3')]\n",
        "q1features_valid_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_valid_1.columns[q1features_valid_1.columns.str.startswith('EDUCA')]\n",
        "q1features_valid_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_valid_1.columns[q1features_valid_1.columns.str.startswith('SLEPTIM1')]\n",
        "q1features_valid_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_valid_1.columns[q1features_valid_1.columns.str.startswith('PRIMINSR')]\n",
        "q1features_valid_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_valid_1.columns[q1features_valid_1.columns.str.startswith('FMONTH')]\n",
        "q1features_valid_1.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_valid_1.columns[q1features_valid_1.columns.str.startswith('SMOKE100')]\n",
        "q1features_valid_1.drop(unwanted, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "u8j_SzZngMnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is imbalanced; a majority of respondents reported that they did not have COVID-19 symptoms that lasted longer than 3 months. Imbalanced data adversely affects model performance; if the model encounters few instances of the minority class then it will be unable to effectively learn from this class.\n",
        "\n",
        "The SMOTE algorithm was used to oversample the minority class; this address the issue of imbalanced data."
      ],
      "metadata": {
        "id": "YJ5Y5crc6QTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SMOTE Algorithm\n",
        "smo = SMOTE(random_state = 2, k_neighbors=10)\n",
        "q1features_train_1_SM, q1target_train_1_SM = smo.fit_resample(q1features_train_1, q1target_train_1.ravel())"
      ],
      "metadata": {
        "id": "vQzCzxDogNyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid search was used to optimize model hyperparameters; the models were run with each possible combination of hyperparameters across all five sliding windows. The values of these hyperparameters were taken from Dr. Aamna AlShehhi and colleague's paper 'Utilizing machine learning for survival analysis to identify risk factors for COVID-19 intensive care unit admission: A retrospective cohort study from the United Arab Emirates' (https://doi.org/10.1371/journal.pone.0291373).\n",
        "\n",
        "The hyperparameters were selected to optimize the models' F2 scores, which is a metric that is the harmonic mean of both precision and recall, although recall is weighted heavier than precision. This metric was also used by Dr. Roman Kessler and colleagues in their study \"Predictive Attributes for Developing Long COVIDâ€”A Study Using Machine Learning and Real-World Data from Primary Care Physicians in Germany\" (https://doi.org/10.3390/jcm12103511). It is logical to also prioritize recall over precision in this data analytics project, as here also there are more severe consequences for false negatives than false positives."
      ],
      "metadata": {
        "id": "yFEi9owJ6T8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "UOhu6K-egQMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameter_score_list = []\n",
        "solver_list = ['liblinear', 'newton-cg', 'newton-cholesky']\n",
        "\n",
        "for s in solver_list:\n",
        "  logr = LogisticRegression(random_state=1, solver=s)\n",
        "  logReg =logr.fit(q1features_train_1_SM, q1target_train_1_SM)\n",
        "  logRegPrediction = logReg.predict(q1features_valid_1)\n",
        "  logRegPrediction = np.where(logRegPrediction > 0.5, 1, 0)\n",
        "  f2_score = fbeta_score(q1target_valid_1, logRegPrediction, beta=2)\n",
        "  hyperparameter_score_list.append((s, f2_score))\n",
        "\n",
        "logRegScores1 = pd.DataFrame(hyperparameter_score_list, columns =['Solver', 'F2_Score'])"
      ],
      "metadata": {
        "id": "7_DdbqzxgUSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes"
      ],
      "metadata": {
        "id": "pC_LGoPLgYgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a Gaussian Classifier\n",
        "model = GaussianNB()"
      ],
      "metadata": {
        "id": "O9YwJ6j9gbnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "nbModel = model.fit(q1features_train_1_SM, q1target_train_1_SM)"
      ],
      "metadata": {
        "id": "KkMKM-o-gcz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making predictions on the validation set\n",
        "nbPrediction = nbModel.predict(q1features_valid_1)"
      ],
      "metadata": {
        "id": "vv0Kod5Fgd49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f2_scoreNB1 = fbeta_score(q1target_valid_1, nbPrediction, beta=2)"
      ],
      "metadata": {
        "id": "fo-ijb5eggXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "THOCRJSHghny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameter_score_list = []\n",
        "n_estimators_list =[25, 50, 75, 100]\n",
        "max_depth_list = [1, 2, 3, 4, 5, 10]\n",
        "criterion_list = ['gini', 'entropy', 'log_loss']\n",
        "\n",
        "for n in n_estimators_list:\n",
        "  for m in max_depth_list:\n",
        "    for c in criterion_list:\n",
        "      rf = RandomForestClassifier(n_estimators=n, max_depth=m, criterion=c, random_state=42)\n",
        "      rf.fit(q1features_train_1_SM, q1target_train_1_SM)\n",
        "      rfPrediction = rf.predict(q1features_valid_1)\n",
        "      f2_score = fbeta_score(q1target_valid_1, rfPrediction, beta=2)\n",
        "      hyperparameter_score_list.append((n, m, c, f2_score))\n",
        "\n",
        "rfScores1 = pd.DataFrame(hyperparameter_score_list, columns =['n_Estimators', 'Max_depth', 'Criterion', 'F2_Score'])"
      ],
      "metadata": {
        "id": "o9x31-RzgjBJ",
        "outputId": "2737fc18-84e5-4c60-997b-25f23d276603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'RandomForestClassifier' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dedc7d77827f>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmax_depth_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcriterion_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1features_train_1_SM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq1target_train_1_SM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mrfPrediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1features_valid_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient-Boosted Trees"
      ],
      "metadata": {
        "id": "XLQk17hog8Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameter_score_list = []\n",
        "n_estimators_list =[25, 50, 75, 100]\n",
        "max_depth_list = [1, 2, 3, 4, 5, 10]\n",
        "subsample_list = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\n",
        "\n",
        "for n in n_estimators_list:\n",
        "  for m in max_depth_list:\n",
        "      for s in subsample_list:\n",
        "        bst = GradientBoostingClassifier(n_estimators=n, max_depth=m, subsample=s, random_state=42).fit(q1features_train_1_SM, q1target_train_1_SM)\n",
        "        bstPrediction = bst.predict(q1features_valid_1)\n",
        "        f2_score = fbeta_score(q1target_valid_1, bstPrediction, beta=2)\n",
        "        hyperparameter_score_list.append((n, m, s, f2_score))\n",
        "\n",
        "bstScores1 = pd.DataFrame(hyperparameter_score_list, columns =['n_Estimators', 'Max_depth', 'Subsample', 'F2_Score'])"
      ],
      "metadata": {
        "id": "-gRLWOgyg9bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logRegScores1.to_pickle('logRegScores1.pickle')\n",
        "f2_scoreNB1.to_pickle('f2_scoreNB1.pickle')\n",
        "rfScores1.to_pickle('rfScores1.pickle')\n",
        "bstScores1.to_pickle('bstScores1.pickle')"
      ],
      "metadata": {
        "id": "YhvBfCXOhUYS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}