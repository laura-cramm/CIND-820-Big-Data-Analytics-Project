{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78zhmRJxmCFb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import statistics\n",
        "import random\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q1features_trainvalid = pd.read_pickle(\"q1features_trainvalid.pickle\")\n",
        "q1target_trainvalid = pd.read_pickle(\"q1target_trainvalid.pickle\")\n",
        "q1features_test = pd.read_pickle(\"q1features_test.pickle\")\n",
        "q1target_test = pd.read_pickle(\"q1target_test.pickle\")"
      ],
      "metadata": {
        "id": "FGkExeFNsN51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attributes with zero variance were removed from the model; these attributes did not contribute any information to the model."
      ],
      "metadata": {
        "id": "eXOaVZsFAQ33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing zero variance attributes (training set)\n",
        "q1features_trainvalid = q1features_trainvalid.loc[:, q1features_trainvalid.nunique(axis=0) != 1]"
      ],
      "metadata": {
        "id": "hXYgYOWlmcxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing zero variance attributes (test set)\n",
        "q1features_test = q1features_test.loc[:, q1features_test.nunique(axis=0) != 1]"
      ],
      "metadata": {
        "id": "SuE7APPTmhoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attributes with more than 10% missing data were dropped from the dataset. Dr. Iris Eekhout and colleagues in their 2013 paper titled \"Missing data in a multi-item instrument were best handled by multiple imputation at the item score level\" (https://doi.org/10.1016/j.jclinepi.2013.09.009) found that single-imputation methods can result in biased estimates when the dataset has a higher proportion of missing data. Single-imputation methods will be applied later in the analysis, hence the removal of these attributes with a high proportion of missing data at this step."
      ],
      "metadata": {
        "id": "IyNnt6xZAVFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing attributes with more than 10% missing data (training set)\n",
        "for col in q1features_trainvalid.columns.tolist():\n",
        "  if q1features_trainvalid[col].isna().sum()/len(q1features_trainvalid[col]) > 0.10:\n",
        "    q1features_trainvalid = q1features_trainvalid.drop([col], axis=1)"
      ],
      "metadata": {
        "id": "esLhFpI4mrQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing attributes with more than 10% missing data (test set)\n",
        "for col in q1features_test.columns.tolist():\n",
        "  if q1features_test[col].isna().sum()/len(q1features_test[col]) > 0.10:\n",
        "    q1features_test = q1features_test.drop([col], axis=1)"
      ],
      "metadata": {
        "id": "Hq-40R_qmuJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputation of missing data was performed. The alternative was to use complete case analysis, which involves removing from the dataset any rows that were missing any attributes. Complete case analysis leads to a loss of information and can lead to bias if the missing data is not missing completely at random.\n",
        "\n",
        "Single-imputation methods were used; missing categorical values were imputed using the mode of that attribute and missing numeric values were imputed using the median. The test data set was imputed using the mode and median values of the training data set. To avoid data leakage, the training data set was not imputed using test set data.\n"
      ],
      "metadata": {
        "id": "P8FKwziwAY1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imputing test set\n",
        "for col in q1features_test.columns.tolist():\n",
        "  q1features_test[col] = q1features_test[col].replace([None], np.nan)\n",
        "  if (q1features_test[col].dtype == 'category' or q1features_trainvalid[col].dtype =='datetime64[ns]'):\n",
        "    q1features_test[col]= q1features_test[col].fillna(random.choice(statistics.multimode(q1features_trainvalid[col])))\n",
        "  if (q1features_test[col].dtype == 'Int64' or q1features_test[col].dtype == 'int64'):\n",
        "    q1features_test[col] = q1features_test[col].astype('float64')\n",
        "  if (q1features_test[col].dtype == 'float64'):\n",
        "    q1features_test[col] = q1features_test[col].fillna(q1features_trainvalid[col].median())"
      ],
      "metadata": {
        "id": "Zvi5rE3Rnd8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Imputing training set\n",
        "for col in q1features_trainvalid.columns.tolist():\n",
        "  q1features_trainvalid[col] = q1features_trainvalid[col].replace([None], np.nan)\n",
        "  if (q1features_trainvalid[col].dtype == 'category' or q1features_trainvalid[col].dtype =='datetime64[ns]'):\n",
        "    q1features_trainvalid[col]= q1features_trainvalid[col].fillna(random.choice(statistics.multimode(q1features_trainvalid[col])))\n",
        "  if (q1features_trainvalid[col].dtype == 'Int64' or q1features_trainvalid[col].dtype == 'int64'):\n",
        "    q1features_trainvalid[col] = q1features_trainvalid[col].astype('float64')\n",
        "  if (q1features_trainvalid[col].dtype == 'float64'):\n",
        "    q1features_trainvalid[col] = q1features_trainvalid[col].fillna(q1features_trainvalid[col].median())"
      ],
      "metadata": {
        "id": "HdZe3Oydm_fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numeric attributes were standardized; variable importance will be assessed using regression models, and standardization allows for easier interpretation of the model coefficients. Standardization was performed instead of normalization, as standardization is more resistant to outliers."
      ],
      "metadata": {
        "id": "vYI61D3DAjl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardization  of test set\n",
        "for col in q1features_test.columns.tolist():\n",
        "  if (q1features_test[col].dtype == 'Int64' or q1features_test[col].dtype == 'int64' or q1features_test[col].dtype == 'float64'):\n",
        "    for i in range(len(q1features_test[col])):\n",
        "      q1features_test.loc[q1features_test.index[i], col] = (q1features_test.loc[q1features_test.index[i], col] - q1features_trainvalid[col].mean())/np.std(q1features_trainvalid[col])"
      ],
      "metadata": {
        "id": "KUTwTDTXn_Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardization of training set\n",
        "for col in q1features_trainvalid.columns.tolist():\n",
        "  if (q1features_trainvalid[col].dtype == 'Int64' or q1features_trainvalid[col].dtype == 'int64' or q1features_trainvalid[col].dtype == 'float64'):\n",
        "    for i in range(len(q1features_trainvalid[col])):\n",
        "      q1features_trainvalid.loc[q1features_trainvalid.index[i], col] = (q1features_trainvalid.loc[q1features_trainvalid.index[i], col] - q1features_trainvalid[col].mean())/np.std(q1features_trainvalid[col])"
      ],
      "metadata": {
        "id": "erh8U62enzVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The SMOTE algorithm cannot handle datetime, so converting to number of\n",
        "#days since January 1st, year one\n",
        "for i in range(len(q1features_trainvalid['INTERVIEWDATE'])):\n",
        "  q1features_trainvalid.loc[q1features_trainvalid.index[i], 'INTERVIEWDATE'] =  q1features_trainvalid.loc[q1features_trainvalid.index[i], 'INTERVIEWDATE'].toordinal()\n",
        "\n",
        "for i in range(len(q1features_test['INTERVIEWDATE'])):\n",
        "  q1features_test.loc[q1features_test.index[i], 'INTERVIEWDATE'] =  q1features_test.loc[q1features_test.index[i], 'INTERVIEWDATE'].toordinal()\n",
        "\n",
        "q1features_trainvalid['INTERVIEWDATE'] = q1features_trainvalid['INTERVIEWDATE'].astype('float64')\n",
        "q1features_test['INTERVIEWDATE'] = q1features_test['INTERVIEWDATE'].astype('float64')"
      ],
      "metadata": {
        "id": "e-nL-lLKo5bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting categorical variables to dummy variables\n",
        "for col in q1features_trainvalid.columns.tolist():\n",
        "  if (q1features_trainvalid[col].dtype == 'category'):\n",
        "    q1features_trainvalid = pd.concat([q1features_trainvalid, pd.get_dummies(q1features_trainvalid[col], prefix=col, drop_first=True, dtype='float')], axis=1)\n",
        "    q1features_trainvalid = q1features_trainvalid.drop([col], axis=1)"
      ],
      "metadata": {
        "id": "MGl0XeoppPmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in q1features_test.columns.tolist():\n",
        "  if (q1features_test[col].dtype == 'category'):\n",
        "    q1features_test = pd.concat([q1features_test, pd.get_dummies(q1features_test[col], prefix=col, drop_first=True, dtype='float')], axis=1)\n",
        "    q1features_test = q1features_test.drop([col], axis=1)"
      ],
      "metadata": {
        "id": "4VVfd2_3pYG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1target_trainvalid = q1target_trainvalid.astype('bool')\n",
        "q1target_trainvalid = q1target_trainvalid.astype('float')"
      ],
      "metadata": {
        "id": "9R8WvS7kpkR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1target_test = q1target_test.astype('bool')\n",
        "q1target_test = q1target_test.astype('float')"
      ],
      "metadata": {
        "id": "0Dm-taKXpn2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multicollinearity can lead to model overfitting and can artificially hide the importance of explanatory variables. Multicollinearity (and collinearity) can be assessed using variance inflation factor values. A VIF value of over 10 indicates serious multicollinearity (Vittinghoff, E., Shiboski, S., Glidden, D., & McCulloch, C. (2004). Regression Methods in Biostatistics: Linear, Logistic, Survival and Repeated Measures Models. New York:Springer. https://doi.org/10.1007/b138825). Independent variables were iteratively removed from the model starting with the variable with the highest VIF value until all VIF values were below an accceptable threshold."
      ],
      "metadata": {
        "id": "Z5jDB-1dAmEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing highly correlated variables\n",
        "unwanted = q1features_trainvalid.columns[q1features_trainvalid.columns.str.startswith('IDAY')]\n",
        "q1features_trainvalid.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_trainvalid.columns[q1features_trainvalid.columns.str.startswith('QSTVER')]\n",
        "q1features_trainvalid.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_trainvalid.columns[q1features_trainvalid.columns.str.startswith('STATE')]\n",
        "q1features_trainvalid.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_trainvalid.columns[q1features_trainvalid.columns.str.startswith('_AGE65YR')]\n",
        "q1features_trainvalid.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_trainvalid.columns[q1features_trainvalid.columns.str.startswith('IMONTH')]\n",
        "q1features_trainvalid.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_trainvalid.columns[q1features_trainvalid.columns.str.startswith('INTERVIEWDATE')]\n",
        "q1features_trainvalid.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_trainvalid.columns[q1features_trainvalid.columns.str.startswith('_AGE_G')]\n",
        "q1features_trainvalid.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_trainvalid.columns[q1features_trainvalid.columns.str.startswith('_AGE80')]\n",
        "q1features_trainvalid.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_trainvalid.columns[q1features_trainvalid.columns.str.startswith('HTM4')]\n",
        "q1features_trainvalid.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_trainvalid.columns[q1features_trainvalid.columns.str.startswith('WTKG3')]\n",
        "q1features_trainvalid.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_trainvalid.columns[q1features_trainvalid.columns.str.startswith('USENOW3')]\n",
        "q1features_trainvalid.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_trainvalid.columns[q1features_trainvalid.columns.str.startswith('EDUCA')]\n",
        "q1features_trainvalid.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_trainvalid.columns[q1features_trainvalid.columns.str.startswith('SLEPTIM1')]\n",
        "q1features_trainvalid.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_trainvalid.columns[q1features_trainvalid.columns.str.startswith('PRIMINSR')]\n",
        "q1features_trainvalid.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_trainvalid.columns[q1features_trainvalid.columns.str.startswith('FMONTH')]\n",
        "q1features_trainvalid.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_trainvalid.columns[q1features_trainvalid.columns.str.startswith('SMOKE100')]\n",
        "q1features_trainvalid.drop(unwanted, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "pP2s0dQYpzM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unwanted = q1features_test.columns[q1features_test.columns.str.startswith('IDAY')]\n",
        "q1features_test.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_test.columns[q1features_test.columns.str.startswith('QSTVER')]\n",
        "q1features_test.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_test.columns[q1features_test.columns.str.startswith('STATE')]\n",
        "q1features_test.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_test.columns[q1features_test.columns.str.startswith('_AGE65YR')]\n",
        "q1features_test.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_test.columns[q1features_test.columns.str.startswith('IMONTH')]\n",
        "q1features_test.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_test.columns[q1features_test.columns.str.startswith('INTERVIEWDATE')]\n",
        "q1features_test.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_test.columns[q1features_test.columns.str.startswith('_AGE_G')]\n",
        "q1features_test.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_test.columns[q1features_test.columns.str.startswith('_AGE80')]\n",
        "q1features_test.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_test.columns[q1features_test.columns.str.startswith('HTM4')]\n",
        "q1features_test.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_test.columns[q1features_test.columns.str.startswith('WTKG3')]\n",
        "q1features_test.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_test.columns[q1features_test.columns.str.startswith('USENOW3')]\n",
        "q1features_test.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_test.columns[q1features_test.columns.str.startswith('EDUCA')]\n",
        "q1features_test.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_test.columns[q1features_test.columns.str.startswith('SLEPTIM1')]\n",
        "q1features_test.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_test.columns[q1features_test.columns.str.startswith('PRIMINSR')]\n",
        "q1features_test.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_test.columns[q1features_test.columns.str.startswith('FMONTH')]\n",
        "q1features_test.drop(unwanted, axis=1, inplace=True)\n",
        "\n",
        "unwanted = q1features_test.columns[q1features_test.columns.str.startswith('SMOKE100')]\n",
        "q1features_test.drop(unwanted, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "8ob8Je85p_rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is imbalanced; a majority of respondents reported that they did not have COVID-19 symptoms that lasted longer than 3 months. Imbalanced data adversely affects model performance; if the model encounters few instances of the minority class then it will be unable to effectively learn from this class.\n",
        "\n",
        "The SMOTE algorithm was used to oversample the minority class; this address the issue of imbalanced data."
      ],
      "metadata": {
        "id": "z9P9BC6ZAtSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SMOTE Algorithm\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smo = SMOTE(random_state = 2, k_neighbors=10)\n",
        "q1features_trainvalid_SM, q1target_trainvalid_SM = smo.fit_resample(q1features_trainvalid, q1target_trainvalid.ravel())"
      ],
      "metadata": {
        "id": "nZU6bOPWqLAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f2_list = []"
      ],
      "metadata": {
        "id": "8uSyuuZBqg6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "4SAMMIFEqT9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logr = LogisticRegression(random_state=1, solver=\"liblinear\" )\n",
        "logReg =logr.fit(q1features_trainvalid_SM, q1target_trainvalid_SM)\n",
        "logRegPrediction = logReg.predict(q1features_test)\n",
        "logRegPrediction = np.where(logRegPrediction > 0.5, 1, 0)\n",
        "f2_score = fbeta_score(q1target_test, logRegPrediction, beta=2)\n",
        "f2_score"
      ],
      "metadata": {
        "id": "zm--5-z5A09m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logRegMatrix = metrics.confusion_matrix(q1target_test, logRegPrediction)\n",
        "logRegMatrix"
      ],
      "metadata": {
        "id": "9phluXLMA3of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The LogisticRegression function of the sklearn library does not\n",
        "#support retrieving the p-values of each independent variable directly.\n",
        "#Using the statsmodels library to retrieve this information\n",
        "import statsmodels.api as sm\n",
        "log_reg = sm.Logit(q1target_trainvalid_SM, q1features_trainvalid_SM).fit()\n",
        "print(log_reg.summary())"
      ],
      "metadata": {
        "id": "9ofzl-taA8z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes"
      ],
      "metadata": {
        "id": "yIiNzKtKq53J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a Gaussian Classifier\n",
        "model = GaussianNB()\n",
        "\n",
        "# Model training\n",
        "nbModel = model.fit(q1features_trainvalid_SM, q1target_trainvalid_SM)\n",
        "\n",
        "#Making predictions on the validation set\n",
        "nbPrediction = nbModel.predict(q1features_test)\n",
        "\n",
        "f2_score = fbeta_score(q1target_test, nbPrediction, beta=2)\n",
        "\n",
        "f2_score"
      ],
      "metadata": {
        "id": "nF1hpxzRA1TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nbMatrix = metrics.confusion_matrix(q1target_test, nbPrediction)\n",
        "nbMatrix"
      ],
      "metadata": {
        "id": "6SqQlenvBDwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "mJv644MzrPOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=100, max_depth=1, criterion=\"log_loss\", random_state=42)\n",
        "rf.fit(q1features_trainvalid_SM, q1target_trainvalid_SM)\n",
        "rfPrediction = rf.predict(q1features_test)\n",
        "f2_score = fbeta_score(q1target_test, rfPrediction, beta=2)\n",
        "f2_score"
      ],
      "metadata": {
        "id": "48AiincaA1vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfMatrix = metrics.confusion_matrix(q1target_test, rfPrediction)\n",
        "rfMatrix"
      ],
      "metadata": {
        "id": "JmvVeCyfBJh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient-Boosted Trees"
      ],
      "metadata": {
        "id": "5nsqZ0njriDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bst = GradientBoostingClassifier(n_estimators=25, max_depth=1, subsample=0.2, random_state=42).fit(q1features_trainvalid_SM, q1target_trainvalid_SM)\n",
        "bstPrediction = bst.predict(q1features_test)\n",
        "f2_score = fbeta_score(q1target_test, bstPrediction, beta=2)\n",
        "f2_score"
      ],
      "metadata": {
        "id": "Vu4Uf-uxA2Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bstMatrix = metrics.confusion_matrix(q1target_test, bstPrediction)\n",
        "bstMatrix"
      ],
      "metadata": {
        "id": "tgWv-r2zBMD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this initial code, the logistic regression model seems to have the best performance. However, the F-2 score is quite low (apprx. 0.49). Among the next steps of this data analysis project will be to further optimize model performance."
      ],
      "metadata": {
        "id": "nfRpH9myBOnP"
      }
    }
  ]
}