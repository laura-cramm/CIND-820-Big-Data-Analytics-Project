{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPsTxZ/WQhu0iGm0xkmSVGm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Plh4qx5QeTq0"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","from datetime import datetime\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import TimeSeriesSplit\n","\n","import statistics\n","import random\n","\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.feature_selection import SelectFromModel\n","\n","from imblearn.over_sampling import SMOTE\n","\n","from sklearn import metrics\n","from sklearn.metrics import fbeta_score\n","\n","import statsmodels.api as sm\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import RandomForestClassifier\n","import lightgbm as lgb"]},{"cell_type":"code","source":["q3features_train_5 = pd.read_pickle(\"q3features_train_5.pickle\")\n","q3features_valid_5 = pd.read_pickle(\"q3features_valid_5.pickle\")\n","q3target_train_5 = pd.read_pickle(\"q3target_train_5.pickle\")\n","q3target_valid_5 = pd.read_pickle(\"q3target_valid_5.pickle\")"],"metadata":{"id":"JqQvk_cku2Wq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Imputation of missing data was performed. The alternative was to use complete case analysis, which involves removing from the dataset any rows that were missing any attributes. Complete case analysis leads to a loss of information and can lead to bias if the missing data is not missing completely at random.\n","\n","Single-imputation methods were used; missing categorical values were imputed using the mode of that attribute and missing numeric values were imputed using the median. The validation data set was imputed using the mode and median values of the training data set. To avoid data leakage, the training data set was not imputed using validation set data. Data leakage can lead to inaccurately high model performance (but comparatively poor performance on the test data)."],"metadata":{"id":"n7F9uhAmvsTB"}},{"cell_type":"code","source":["#Imputing validation set\n","for col in q3features_valid_5.columns.tolist():\n","  q3features_valid_5[col] = q3features_valid_5[col].replace([None], np.nan)\n","  if (q3features_valid_5[col].dtype == 'category' or q3features_valid_5[col].dtype =='datetime64[ns]'): #\n","    q3features_valid_5[col]= q3features_valid_5[col].fillna(random.choice(statistics.multimode(q3features_train_5[col])))\n","  if (q3features_valid_5[col].dtype == 'Int64' or q3features_valid_5[col].dtype == 'int64'):\n","    q3features_valid_5[col] = q3features_valid_5[col].astype('float64')\n","  if (q3features_valid_5[col].dtype == 'float64'):\n","    q3features_valid_5[col] = q3features_valid_5[col].fillna(q3features_train_5[col].median())"],"metadata":{"id":"HRcGbrFtvtrI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Imputing training set\n","for col in q3features_train_5.columns.tolist():\n","  q3features_train_5[col] = q3features_train_5[col].replace([None], np.nan)\n","  if (q3features_train_5[col].dtype == 'category' or q3features_train_5[col].dtype =='datetime64[ns]'): #\n","    q3features_train_5[col] = q3features_train_5[col].fillna(random.choice(statistics.multimode(q3features_train_5[col])))\n","  if (q3features_train_5[col].dtype == 'Int64' or q3features_train_5[col].dtype == 'int64'):\n","    q3features_train_5[col] = q3features_train_5[col].astype('float64')\n","  if (q3features_train_5[col].dtype == 'float64'):\n","    q3features_train_5[col] = q3features_train_5[col].fillna(q3features_train_5[col].median())"],"metadata":{"id":"1P_Xy8Fo2-ix"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Numeric attributes were standardized; variable importance will be assessed using regression models, and standardization allows for easier interpretation of the model coefficients. Standardization was performed instead of normalization, as standardization is more resistant to outliers."],"metadata":{"id":"zG5NJmwo3H-a"}},{"cell_type":"code","source":["#Standardization of validation set\n","for col in q3features_valid_5.columns.tolist():\n","  if (q3features_valid_5[col].dtype == 'Int64' or q3features_valid_5[col].dtype == 'int64' or q3features_valid_5[col].dtype == 'float64'):\n","    for i in range(len(q3features_valid_5[col])):\n","      q3features_valid_5.loc[q3features_valid_5.index[i], col] = (q3features_valid_5.loc[q3features_valid_5.index[i], col] - q3features_train_5[col].mean())/np.std(q3features_train_5[col])"],"metadata":{"id":"Wa9b_9vV3JHp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Standardization  of training set\n","for col in q3features_train_5.columns.tolist():\n","  if (q3features_train_5[col].dtype == 'Int64' or q3features_train_5[col].dtype == 'int64' or q3features_train_5[col].dtype == 'float64'):\n","    for i in range(len(q3features_train_5[col])):\n","      q3features_train_5.loc[q3features_train_5.index[i], col] = (q3features_train_5.loc[q3features_train_5.index[i], col] - q3features_train_5[col].mean())/np.std(q3features_train_5[col])"],"metadata":{"id":"zsuey3pZ3Qtd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#The SMOTE algorithm cannot handle datetime, so converting to number of\n","#days since January 1st, year one\n","for i in range(len(q3features_train_5['INTERVIEWDATE'])):\n","  q3features_train_5.loc[q3features_train_5.index[i], 'INTERVIEWDATE'] =  q3features_train_5.loc[q3features_train_5.index[i], 'INTERVIEWDATE'].toordinal()\n","\n","for i in range(len(q3features_valid_5['INTERVIEWDATE'])):\n","  q3features_valid_5.loc[q3features_valid_5.index[i], 'INTERVIEWDATE'] =  q3features_valid_5.loc[q3features_valid_5.index[i], 'INTERVIEWDATE'].toordinal()\n","\n","q3features_train_5['INTERVIEWDATE'] = q3features_train_5['INTERVIEWDATE'].astype('float64')\n","q3features_valid_5['INTERVIEWDATE'] = q3features_valid_5['INTERVIEWDATE'].astype('float64')"],"metadata":{"id":"Z4Iy9BHj3ZFB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Converting categorical variables to dummy variables\n","for col in q3features_train_5.columns.tolist():\n","  if (q3features_train_5[col].dtype == 'category'):\n","    q3features_train_5 = pd.concat([q3features_train_5, pd.get_dummies(q3features_train_5[col], prefix=col, drop_first=True, dtype='float')], axis=1)\n","    q3features_train_5 = q3features_train_5.drop([col], axis=1)"],"metadata":{"id":"eVtbbrgz3hdI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for col in q3features_valid_5.columns.tolist():\n","  if (q3features_valid_5[col].dtype == 'category'):\n","    q3features_valid_5 = pd.concat([q3features_valid_5, pd.get_dummies(q3features_valid_5[col], prefix=col, drop_first=True, dtype='float')], axis=1)\n","    q3features_valid_5 = q3features_valid_5.drop([col], axis=1)"],"metadata":{"id":"2bzBBSCp3mgp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["q3target_train_5 = q3target_train_5.astype('bool')\n","q3target_train_5 = q3target_train_5.astype('float')"],"metadata":{"id":"_JexpEHA3q1y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["q3target_valid_5 = q3target_valid_5.astype('bool')\n","q3target_valid_5 = q3target_valid_5.astype('float')"],"metadata":{"id":"7S8egE8A3s9p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Multicollinearity can lead to model overfitting and can artificially hide the importance of explanatory variables. Multicollinearity (and collinearity) can be assessed using variance inflation factor values. A VIF value of over 10 indicates serious multicollinearity (Vittinghoff, E., Shiboski, S., Glidden, D., & McCulloch, C. (2004). Regression Methods in Biostatistics: Linear, Logistic, Survival and Repeated Measures Models. New York:Springer. https://doi.org/10.1007/b138825). Independent variables were iteratively removed from the model starting with the variable with the highest VIF value until all VIF values were below an accceptable threshold."],"metadata":{"id":"VvKfqZth3vEy"}},{"cell_type":"code","source":["#Removing highly correlated variables\n","vifcalcs=q3features_train_5"],"metadata":{"id":"t9oOdW-13wbx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vif_data = pd.DataFrame()\n","vif_data[\"feature\"] = vifcalcs.columns\n","\n","# calculating VIF for each feature\n","vif_data[\"VIF\"] = [variance_inflation_factor(vifcalcs.values, i)\n","                          for i in range(len(vifcalcs.columns))]"],"metadata":{"id":"VTJboDqk3yXp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range (len(vif_data['VIF'])):\n","  if (vif_data.loc[vif_data.index[i], 'VIF'])> 10:\n","    print(vif_data.loc[vif_data.index[i], 'feature'],vif_data.loc[vif_data.index[i], 'VIF'])"],"metadata":{"id":"927gASF63zRt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#unwanted = vifcalcs.columns[vifcalcs.columns.str.startswith('PRIMINSR')]\n","#vifcalcs.drop(unwanted, axis=1, inplace=True)"],"metadata":{"id":"2hwesDep30J_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["unwanted = q3features_train_5.columns[q3features_train_5.columns.str.startswith('STATE')]\n","q3features_train_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_train_5.columns[q3features_train_5.columns.str.startswith('_AGE65YR')]\n","q3features_train_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_train_5.columns[q3features_train_5.columns.str.startswith('QSTVER')]\n","q3features_train_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_train_5.columns[q3features_train_5.columns.str.startswith('IYEAR')]\n","q3features_train_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_train_5.columns[q3features_train_5.columns.str.startswith('INTERVIEWDATE')]\n","q3features_train_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_train_5.columns[q3features_train_5.columns.str.startswith('DISPCODE')]\n","q3features_train_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_train_5.columns[q3features_train_5.columns.str.startswith('SMOKE100_')]\n","q3features_train_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_train_5.columns[q3features_train_5.columns.str.startswith('EDUCA')]\n","q3features_train_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_train_5.columns[q3features_train_5.columns.str.startswith('HTM4')]\n","q3features_train_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_train_5.columns[q3features_train_5.columns.str.startswith('USENOW')]\n","q3features_train_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_train_5.columns[q3features_train_5.columns.str.startswith('SEQNO')]\n","q3features_train_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_train_5.columns[q3features_train_5.columns.str.startswith('FMONTH')]\n","q3features_train_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_train_5.columns[q3features_train_5.columns.str.startswith('_AGE_G')]\n","q3features_train_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_train_5.columns[q3features_train_5.columns.str.startswith('PRIMINSR')]\n","q3features_train_5.drop(unwanted, axis=1, inplace=True)"],"metadata":{"id":"gQ34jGXU31DA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["unwanted = q3features_valid_5.columns[q3features_valid_5.columns.str.startswith('STATE')]\n","q3features_valid_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_valid_5.columns[q3features_valid_5.columns.str.startswith('_AGE65YR')]\n","q3features_valid_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_valid_5.columns[q3features_valid_5.columns.str.startswith('QSTVER')]\n","q3features_valid_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_valid_5.columns[q3features_valid_5.columns.str.startswith('IYEAR')]\n","q3features_valid_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_valid_5.columns[q3features_valid_5.columns.str.startswith('INTERVIEWDATE')]\n","q3features_valid_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_valid_5.columns[q3features_valid_5.columns.str.startswith('DISPCODE')]\n","q3features_valid_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_valid_5.columns[q3features_valid_5.columns.str.startswith('SMOKE100_')]\n","q3features_valid_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_valid_5.columns[q3features_valid_5.columns.str.startswith('EDUCA')]\n","q3features_valid_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_valid_5.columns[q3features_valid_5.columns.str.startswith('HTM4')]\n","q3features_valid_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_valid_5.columns[q3features_valid_5.columns.str.startswith('USENOW')]\n","q3features_valid_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_valid_5.columns[q3features_valid_5.columns.str.startswith('SEQNO')]\n","q3features_valid_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_valid_5.columns[q3features_valid_5.columns.str.startswith('FMONTH')]\n","q3features_valid_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_valid_5.columns[q3features_valid_5.columns.str.startswith('_AGE_G')]\n","q3features_valid_5.drop(unwanted, axis=1, inplace=True)\n","\n","unwanted = q3features_valid_5.columns[q3features_valid_5.columns.str.startswith('PRIMINSR')]\n","q3features_valid_5.drop(unwanted, axis=1, inplace=True)"],"metadata":{"id":"vRUTgLhx4Iz5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Feature selection can help reduce model overfitting. Feature selection can also result in dimensionality reduction, which can improve computational efficiency. Here, a decision tree model was used to evaluate feature importance; this model has a built-in method to compute Gini importance values for each attribute. Gini importance is also referred to as \"mean decrease impurity\" and is a measure of how a given attribute improves the purity of a node. Attributes with a Gini importance value of less than a robust threshold of 0.01 were removed from the model. Note that dummy variables generated using one-hot encoding will be grouped together; if one variable meets the threshold then all of the set of dummy variables will remain in the model. This is because the entire set of dummy variables is required to represent one categorical variable."],"metadata":{"id":"JRw7K1FsQk78"}},{"cell_type":"code","source":["clf = DecisionTreeClassifier(max_depth=16, random_state=8)\n","clf.fit(q3features_train_5, q3target_train_5)\n","y_pred = clf.predict(q3features_valid_5)\n","\n","importances = clf.feature_importances_\n","threshold = 0.01\n","selected_features = q3features_train_5.columns[importances > threshold]\n","selected_features.tolist()"],"metadata":{"id":"STVrb4FWQlyb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filter_col = [col for col in q3features_train_5 if col.startswith('CHILDREN') or col.startswith('CPDEMO1C')\n","or col.startswith('DROCDY4')  or col.startswith('PHYSHLTH')  or col.startswith('SLEPTIM1')  or col.startswith('WTKG3')\n","or col.startswith('_AGE80')  or col.startswith('ASTHMA3')  or col.startswith('COVIDSMP')  or col.startswith('FLUSHOT7')\n","or col.startswith('IMONTH')  or col.startswith('PERSDOC3')  or col.startswith('PNEUVAC4')  or col.startswith('SMOKER3')  ]"],"metadata":{"id":"YZ8ErciVQl3i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["q3features_train_5 = q3features_train_5[filter_col]\n","q3features_valid_5 = q3features_valid_5[filter_col]"],"metadata":{"id":"62HMQEzYQl9f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The dataset is imbalanced; a majority of respondents reported that they did not have COVID-19 symptoms that lasted longer than 3 months. Imbalanced data adversely affects model performance; if the model encounters few instances of the minority class then it will be unable to effectively learn from this class.\n","\n","The SMOTE algorithm was used to oversample the minority class; this address the issue of imbalanced data."],"metadata":{"id":"RA_MarYu4cEY"}},{"cell_type":"code","source":["#SMOTE Algorithm\n","smo = SMOTE(random_state = 2, k_neighbors=10)\n","q3features_train_5_SM, q3target_train_5_SM = smo.fit_resample(q3features_train_5, q3target_train_5.ravel())"],"metadata":{"id":"7J5wszKb4dSo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Grid search was used to optimize model hyperparameters; the models were run with each possible combination of hyperparameters across all five sliding windows. The values of these hyperparameters were taken from Dr. Aamna AlShehhi and colleague's paper 'Utilizing machine learning for survival analysis to identify risk factors for COVID-19 intensive care unit admission: A retrospective cohort study from the United Arab Emirates' (https://doi.org/10.1371/journal.pone.0291373).\n","\n","The hyperparameters were selected to optimize the models' F2 scores, which is a metric that is the harmonic mean of both precision and recall, although recall is weighted heavier than precision. This metric was also used by Dr. Roman Kessler and colleagues in their study \"Predictive Attributes for Developing Long COVID—A Study Using Machine Learning and Real-World Data from Primary Care Physicians in Germany\" (https://doi.org/10.3390/jcm12103511). It is logical to also prioritize recall over precision in this data analytics project, as here also there are more severe consequences for false negatives than false positives."],"metadata":{"id":"vDoER2c14glf"}},{"cell_type":"markdown","source":["Logistic Regression"],"metadata":{"id":"n_aCvWxX4huo"}},{"cell_type":"code","source":["hyperparameter_score_list = []\n","threshold_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n","\n","for p in threshold_list:\n","  logReg = sm.Logit(q3target_train_5_SM.ravel(), q3features_train_5_SM,).fit()\n","  logRegPrediction = logReg.predict(q3features_valid_5)\n","  logRegPrediction = np.where(logRegPrediction > p, 1, 0)\n","\n","  f2_score = fbeta_score(q3target_valid_5, logRegPrediction, beta=2)\n","  accuracy =  metrics.accuracy_score(q3target_valid_5, logRegPrediction)\n","  recall =  metrics.recall_score(q3target_valid_5, logRegPrediction)\n","  precision = metrics.precision_score(q3target_valid_5, logRegPrediction)\n","  hyperparameter_score_list.append((p, f2_score, accuracy, recall, precision))\n","\n","logRegScores5 = pd.DataFrame(hyperparameter_score_list, columns =['Threshold', 'F2_Score', 'Accuracy', 'Recall', 'Precision'])"],"metadata":{"id":"4m0ekhDZ4jN7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logRegScores5.to_csv('logRegScores5.csv')"],"metadata":{"id":"VnJkJfkUPsWS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Naive Bayes"],"metadata":{"id":"be8JTEEB4nnA"}},{"cell_type":"code","source":["hyperparameter_score_list = []\n","threshold_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n","\n","for p in threshold_list:\n","  model = GaussianNB()\n","  nbModel = model.fit(q3features_train_5_SM, q3target_train_5_SM)\n","  nbPrediction = (nbModel.predict_proba(q3features_valid_5)[:,1] >= p).astype(bool)\n","\n","  f2_score = fbeta_score(q3target_valid_5, nbPrediction, beta=2)\n","  accuracy =  metrics.accuracy_score(q3target_valid_5, nbPrediction)\n","  recall =  metrics.recall_score(q3target_valid_5, nbPrediction)\n","  precision = metrics.precision_score(q3target_valid_5, nbPrediction)\n","  hyperparameter_score_list.append((p, f2_score, accuracy, recall, precision))\n","\n","nbScores5 = pd.DataFrame(hyperparameter_score_list, columns =['Threshold', 'F2_Score', 'Accuracy', 'Recall', 'Precision'])"],"metadata":{"id":"YUls8O3baj73"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nbScores5.to_csv('nbScores5.csv')"],"metadata":{"id":"aLy77J72Pr0Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Random Forest"],"metadata":{"id":"beTFC5gU4wGZ"}},{"cell_type":"code","source":["hyperparameter_score_list = []\n","n_estimators_list =[25, 50, 75, 100]\n","max_depth_list = [1, 2, 3, 4, 5, 10]\n","threshold_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n","\n","for n in n_estimators_list:\n","  for m in max_depth_list:\n","    for p in threshold_list:\n","      rf = RandomForestClassifier(n_estimators=n, max_depth=m,random_state=42)\n","      rf.fit(q3features_train_5_SM, q3target_train_5_SM)\n","      #rfPrediction = rf.predict(q3features_valid_5)\n","      rfPrediction = (rf.predict_proba(q3features_valid_5)[:,1] >= p).astype(bool)\n","\n","      f2_score = fbeta_score(q3target_valid_5, rfPrediction, beta=2)\n","      accuracy =  metrics.accuracy_score(q3target_valid_5, rfPrediction)\n","      recall =  metrics.recall_score(q3target_valid_5, rfPrediction)\n","      precision = metrics.precision_score(q3target_valid_5, rfPrediction)\n","      hyperparameter_score_list.append((n, m, p, f2_score, accuracy, recall, precision))\n","\n","rfScores5 = pd.DataFrame(hyperparameter_score_list, columns =['n_Estimators', 'Max_depth', 'Threshold', 'F2_Score', 'Accuracy', 'Recall', 'Precision'])"],"metadata":{"id":"UmGQ0F164w2I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rfScores5.to_csv('rfScores5.csv')"],"metadata":{"id":"ztjfiR1bPrUC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Light GBM"],"metadata":{"id":"FB-snDnCz_Hl"}},{"cell_type":"code","source":["train_data = lgb.Dataset(q3features_train_5_SM, label=q3target_train_5_SM)\n","test_data = lgb.Dataset(q3features_valid_5, label=q3target_valid_5, reference=train_data)\n","\n","hyperparameter_score_list = []\n","num_leaves_list = [7, 31]\n","reg_alpha_list = [0.1, 0.5]\n","reg_lambda_list = [0.1, 0.5]\n","min_data_list = [5, 30, 100]\n","threshold_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n","\n","\n","for n in num_leaves_list:\n","  for a in reg_alpha_list:\n","    for l in reg_lambda_list:\n","      for d in min_data_list:\n","        for p in threshold_list:\n","          params = {\n","              \"num_leaves\": n,\n","              \"reg_alpha\": a,\n","              \"reg_lambda\": l,\n","              \"min_data_in_leaf\": d,\n","              \"objective\": \"binary\",\n","              \"metric\": \"binary_logloss\",\n","              \"learning_rate\": 0.05,\n","              \"force_row_wise\": True,\n","              \"bagging_fraction\": 0.8,\n","              \"feature_fraction\": 0.8\n","              }\n","          num_round=500\n","          bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n","          y_pred = bst.predict(q3features_valid_5)\n","          y_pred_binary = (y_pred > p).astype(int)\n","\n","          f2_score = fbeta_score(q3target_valid_5, y_pred_binary, beta=2)\n","          accuracy =  metrics.accuracy_score(q3target_valid_5, y_pred_binary)\n","          recall =  metrics.recall_score(q3target_valid_5, y_pred_binary)\n","          precision = metrics.precision_score(q3target_valid_5, y_pred_binary)\n","\n","          hyperparameter_score_list.append((n, a, l, d, p, f2_score, accuracy, recall, precision))\n","\n","bstScores5 = pd.DataFrame(hyperparameter_score_list, columns = ['Num_Leaves', 'Reg_Alpha', 'Reg_Lambda', 'Min_Data', 'Threshold', 'F2_Score', 'Accuracy', 'Recall', 'Precision'])"],"metadata":{"id":"7LdZ8bgB0KRN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bstScores5.to_csv('bstScores5.csv')"],"metadata":{"id":"vsXcNIhCPqth"},"execution_count":null,"outputs":[]}]}